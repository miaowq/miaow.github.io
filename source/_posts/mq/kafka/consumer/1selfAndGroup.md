---
title: 消费者｜消费者与消费组
categories:
  - mq
tags:
  - mq
  - kafka
abbrlink: c9dc6a3b
date: 2023-12-08 23:00:00
---

> 本系列内容基于 kafka_2.12-2.3.1

# 消费者和消费者组
消费者（Consumer）负责订阅 Kafka 中的主题（Topic），并且从订阅的主题上拉取消息。

与其他一些消息中间件不同的是：在 Kafka 的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组（哪怕没有显示指定）。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。

## 举个例子

![](/images/mq/kafka/kafka16.png)

<!-- more -->

如上图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。

有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。

按照 Kafka 默认的规则（即 RangeAssgnor），最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，**两个消费组之间互不影响**。

**每个消费者只能消费所分配到的分区中的消息**。换言之，**每一个分区只能被一个消费组中的一个消费者所消费（但不绝对，参考3.4）**。

# 横向伸缩

假设目前某消费组内只有一个消费者C0，订阅了一个主题，这个主题包含7个分区：P0、P1、P2、P3、P4、P5、P6。也就是说，这个消费者C0订阅了7个分区，参考下图：

![](/images/mq/kafka/kafka17.png)

此时消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0的部分分区分配给消费者C1消费，如下图图所示。

**消费者C0和C1各自负责消费所分配到的分区，彼此之间并无逻辑上的干扰**。

![](/images/mq/kafka/kafka18.png)

紧接着消费组内又加入了一个新的消费者C2，消费者C0、C1和C2按照上图中的方式各自负责消费所分配到的分区。如下图所示：

![](/images/mq/kafka/kafka19.png)

因此，`消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力`。

但，要注意的是：对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。参考下图：一共有8个消费者，7个分区，那么最后的消费者C7由于分配不到任何分区而无法消费任何消息。

![](/images/mq/kafka/kafka20.png)

以上分配逻辑都是基于默认的分区分配策略进行分析的（也就是RangeAssignor），可以通过消费者客户端参数 `partition.assignment.strategy` 来设置消费者与订阅主题之间的分区分配策略。

# 分区分配策略

## RangeAssgnor

### 原理

是按照消费者总数和分区总数进行整除运算来获得一个 `跨度`，然后将分区按照 `跨度` 进行平均分配， 以保证分区尽可能均匀地分配给所有的消费者。对于每一个主题 RangeAssgnor 策略会将消费组内所有订阅这个主题的消费者按照名称的字典序排序，然后为每个消费者划分固定的分区范围，如果不够平均分配，那么字典序靠前的消费者会被多分配一个分区。


### 举例
假设，n = 分区数 ／ 消费者数量， m = 分区数 % 消费者数量，那么前m个消费者每个分配 n + 1分区，后面的（消费者数量－ m）个消费者每个分配n个分区:

2个主题T0和T1，3个分区P0、P1和P2，2个消费者C0和C1

```
C0 T0P0 T0P1 T1P0 T1P1
C1 T0P2 T1P2
```

### 结论

明显这样的分配并不均匀，如果将类似的情形扩大，则有可能出现部分消费者过载的情况。对此我们再来看另一种 RoundRobinAssgnor 策略的分配效果如何。

## RoundRobinAssgnor

### 原理

将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。


### 举例

3个消费者C0、C1和C2，3个主题T0、共订阅了T1和T2，3个主题，且它们分别有1、2、3个分区，经过 RoundRobinAssgnor 策略分配后：

```
C0 T0P0
C1 T1P0
C2 T1P1 T2P0 T2P1 T2P2
```

### 结论

可以看 到 RoundRobinAssignor 策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区 T1P1 分配给消费者 C1。

## StickyAssignor

"sticky"这个单词可以翻译为"黏性的"，Kafka 从 0.11.x 版本开始引入这种分配策略，它主要有两个目的：
1. 分区的分配要尽可能均匀。
2. 分区的分配尽可能与上次分配的保持相同。

### 原理

当上述2个目标发生冲突时，第一个目标优先于第二个目标。


### 举例
3个消费者C0、C1和C2，3个主题T0、共订阅了T1和T2，3个主题，且它们分别有2个分区，经过 StickyAssignor 策略分配后：

```
C0 T0P0 T1P1 T3P0
C1 T0P1 T2P0 T3P1
C2 T1P0 T2P1
```

这样初看上去似乎与采用 RoundRobinAssignor 分配策略所分配的结果相同，但事实是否真的如此呢？

再假设此时消费者 C1 脱离了消费组，那么消费组就会执行再均衡操作，进而消费分区会重新分配。如果采用 RoundRobinAssignor 分配策略，那么此时的分配结果如下：

```
C0 T0P0 T1P0 T2P0 T3P0
C2 T0P1 T1P1 T2P1 T3P1
```
如分配结果所示，RoundRobinAssignor 分配策略会按照消费者 C0 和 C2 进行重新轮询分配。

如果此时使用的是 StickyAssignor 分配策略，那么分配结果为：
```
C0 T0P0 T1P1 T3P0 T2P0
C2 T1P0 T2P1 T0P1 T3P1
```

可以看到分配结果中保留了上一次分配中对消费者 C0 和 C2 的所有分配结果，并将原来消费者 C1 的“负担”分配给了剩余的两个消费者 C0 和 C2，最终 C0 和 C2 的分配还保持了均衡。

### 结论

如果发生分区重分配，那么对于同一个分区而言，有可能之前的消费者和新指派的消费者不是同一个，之前消费者进行到一半的处理还要在新指派的消费者中再次复现一遍，这显然很浪费系统资源。StickyAssignor 分配策略如同其名称中的“sticky”一样，让分配策略具备一定的“黏性”，尽可能地让前后两次分配相同，进而减少系统资源的损耗及其他异常情况的发生。

## 自定义分区分配策略

笔者实际中还没有使用，以下内容为摘录（基于Java）：

定义的分配策略必须要实现 org.apache.kafka.clients.consumer.internals.PartitionAssignor 接口。

PartitionAssignor 接口中定义了两个内部类：Subscription 和 Assignment。

Subscription 类用来表示消费者的订阅信息，类中有两个属性：topics 和 userData，分别表示消费者的订阅主题列表和用户自定义信息。PartitionAssignor 接口通过 subscription() 方法来设置消费者自身相关的 Subscription 信息，注意到此方法中只有一个参数 topics，与 Subscription 类中的 topics 的相呼应，但并没有体现有关 userData 的参数。为了增强用户对分配结果的控制，可以在 subscription() 方法内部添加一些影响分配的用户自定义信息赋予 userData，比如权重、IP 地址、host 或机架（rack）等。

![](/images/mq/kafka/kafka21.png)

举个例子，在 subscription() 方法中提供机架信息，标识此消费者所部署的机架位置，在分区分配时可以根据分区的 leader 副本所在的机架位置来实施具体的分配，这样可以让消费者与所需拉取消息的 broker 节点处于同一机架。

参考上图，消费者 consumer1 和 broker1 都部署在机架 rack1 上，消费者 consumer2 和 broker2 都部署在机架 rack2 上。如果分区的分配不是机架感知的，那么有可能与上图（上半部分）中的分配结果一样，consumer1 消费 broker2 中的分区，而 consumer2 消费 broker1 中的分区；如果分区的分配是机架感知的，那么就会出现上图（下半部分）的分配结果，consumer1 消费 broker1 中的分区，而 consumer2 消费 broker2 中的分区，这样相比前一种情形，既可以减少消费延时，又可以减少跨机架带宽的占用。

# 几个问题

**1. 为什么分区可以提高并行性和可伸缩性？**

每个主题都被划分为若干个分区，每个分区可以由不同的消费者并行消费。通过将消息路由到不同的分区，Kafka可以在集群中的多个节点上并行处理消息，提高整体的吞吐量。

**2. 一个分区是否只能被同一个消费组内的一个消费者消费？**

不一定。当使用`自定义分区分配策略`时，可以使一个分区可以分配给多个消费者消费。如下图：

![](/images/mq/kafka/kafka22.png)

上图代表了一种极端情况：同一消费组内的任意消费者都可以消费订阅主题的所有分区，从而实现了一种"组内广播（消费）"的功能。

但需要注意：组内广播的这种实现方式会有一个严重的问题—默认的消费位移的提交会失效。所有的消费者都会提交它自身的消费位移到 __consumer_offsets 中，后提交的消费位移会覆盖前面提交的消费位移：

- 消费者 consumer1 提交了分区 tp0 的消费位移为10，这时消费者 consumer2 紧接着提交了同一分区 tp0 的消费位移为12，如果此时消费者 consumer1 由于某些原因重启了，那么 consumer1 就会从位移12之后重新开始消费，这样 consumer1 就丢失了部分消息。

- 消费者 consumer1 提交了分区 tp0 的消费位移为10，这时消费者 consumer2 紧接着提交了同一分区的消费位移为8，如果此时消费者 consumer1 由于某些原因重启了，那么 consumer1 就会从位移8之后重新开始消费，这样 consumer1 就重复消费了消息。

很多情形下，重复消费少量消息对于上层业务应用来说可以忍受。但如果：消费位移到了10000了，一个新的消费者加入，它消费了一些消息，提交了消费位移10，那么就会有大量的消息被重复消费，会让上层业务应用猝不及防，同样会造成计算资源的浪费。因此，可以有如下的改进：需要自己保存每个消费者的消费位移。比如可以通过将消费位移保存到本地文件或数据库中等方法来实现组内广播的位移提交。

**3. “消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果正确，那么有没有什么hack的手段？**

- 增加分区数： 可以通过修改主题的分区数来适应更多的消费者。但是，这可能会导致一些副作用，例如可能会影响分区键的一致性。
- 手动分配分区：可以通过手动分配分区的方式来解决。在Kafka中，您可以通过设置assign而不是subscribe来手动指定消费者分区的分配。这样，您可以确保每个消费者只处理其分配的分区，而不受分区总数的限制。