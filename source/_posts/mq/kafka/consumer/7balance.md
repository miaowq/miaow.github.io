---
title: 消费者｜再均衡器
categories:
  - mq
tags:
  - mq
  - kafka
abbrlink: 84be6dcf
date: 2023-12-08 23:00:06
---

> 本系列内容基于 kafka_2.12-2.3.1

# 概念

`再均衡` 是指 分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。

# 两个现象

**1. 在再均衡发生期间，消费组内的消费者是无法读取消息的。**

也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。

**2. 当一个分区被重新分配给另一个消费者时，消费者当前的状态也会丢失。**

比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生。

<!-- more -->

# 消费者协调器 和 组协调器

在讨论之前，先考虑几个问题：

**1. 如果消费者客户端中配置了两个分配策略，那么以哪个为准呢？**

**2. 如果有多个消费者，彼此所配置的分配策略并不完全相同，那么以哪个为准？**

**3. 多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样的呢？**

这一切都是交由 `消费者协调器 ConsumerCoordinator` 和 `组协调器 GroupCoordinator` 来完成的，它们之间使用一套 `组协调协议` 进行交互。

## 旧版消费者客户端的问题

上述的 2个概念是针对新版的消费者客户端而言的，Kafka建立之初并没有它们。

旧版的消费者客户端是使用 `ZooKeeper` 的监听器 `Watcher` 来实现这些功能的。

每个 `group` 在 `ZooKeeper` 中都维护了一个 `／consumers/<group>/ids` 路径，在此路径下使用临时节点记录隶属于此消费组的消费者的唯一标识，它是在消费者启动时创建。

消费者的唯 标识由 consumer.id ＋ 主机名 ＋ 时间戳 ＋ UUID 的部分信息构成，其中 consumer.id 是旧版消者客户端中的配置，相当于新版客户端中的 client.id。

比如某个消费者的唯一标识为 consumerld localhost-1510734527562-64b377f5，那么么其中 consumerId 为指定的 consumer. id, localhost 为计算机的主机名， 1510734527562 表时间戳，而 64b377f5 表示 UUID 部分信息。

![](/images/mq/kafka/kafka23.png)

参考上图，`／consumers/<group>/ids` 同级的还有两个节点：`owners` 和 `offsets`。

`／consumers/<group>/owners` 记录了分区和消费者的对应关系。
`／consumers/<group>/offsets` 记录了此消费组在分区中对应的消费位移。

每个 broker、主题和分区 在 ZooKeeper 中也都对应一个路径：

`/brokers/ids/<id＞` 记录了host、port 及分配在此 broker 上的主题分区列表
`/brokers/topics/<topic＞` 记录了每个分区的 leader 副本、 ISR 集合等信息
`/brokers/topics/<topic>/partitions/<partition>/state` 记录了当前 leader 副本、 leader_epoch 等信息

每个消费者在启动时都会在 `/consumers/<group>/ids` 和 `／brokers/ids` 路径上注册一个监听器。当`/consumers/<group>/ids` 路径下的子节点发生变化时，表示消费组中的消费者发生了变化；当 `／brokers/ids` 路径下的子节点发生变化时，表示 broker 出现了增减。这样通过 ZooKeeper 提供的 Watcher，每个消费者就可以监昕消费组和 Kafka 集群的状态了。

这种方式下，每个消费者对 ZooKeeper 相关路径分别进行监听，触发再均衡操作时，每个消费组下的`所有消费者`会同时进行再均衡操作，而消费者之间并不知道彼此操作的结果，这样可能导致 Kafka 工作在一个不正确的状态。与此同时，这种严重依赖 ZooKeeper 集群的做法还有两个比较严重的问题：

- 羊群效应 
  ZooKeeper 中一个被监听的节点变化，大量的 Watcher通知被发送到客户端，导致在通知期间的其他操作延迟，也有可能发生类
锁的情况。

- 脑裂问题
消费者进行再均衡操作时每个消费者都与 ZooKeeper通信，以判断 消费者 或 broker变化的情况，由于 ZooKeeper 本身的特性，可能导致在同一时刻个消费者获取的状态不一致，这样会导致异常问题发生。

## 新版本再均衡原理

新版的消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费组的子集在服务端对应一个 组协调器 GroupCoordinator 其进行管理。

它是 Kafka 服务端中用于管理消费组的组件。而消费者客户端中的 ConsumerCoordinator 组件负责与 GroupCoordinator
进行交互。

由于它们的中文、英文名称过长，一下由此代替：

CRC：消费者协调器 ConsumerCoordinator
GPC：组协调器 GroupCoordinator

CRC 与 GPC 之间最重要的职责 负责执行消费者再均衡的操作，包括前面提及的分区分配的工作也是在再均衡期间完成的。就目前而言 一共有如下几种情形会触发再均衡的操作：

- 有新的消费者加入消费组
- 有消费者宕机下线，消费者并不一定需要真正下线，例如遇到长时间的GC 、网络延迟导致消费者长时间未向 GPC 发送心跳等情况时， GPC 会认为消费者己经下线
- 有消费者主动退出消费组
- 消费组所对应的 GPC 节点发生了变更
- 消费组内所订阅的任一主题或者主题的分区数量发生变化

### 简单的栗子

这里只做泛述

当有消费者加入消费组时，消费者、消费组及组协调器之间会经历以下几个阶段：

第一阶段：FIND_COORDINATOR

消费者需要确定它所属的消费组对应的 GPC 所在的 broker 并创建与该 broker 相互通信的网络连接。

找到的这个节点是负载最小的节点，即 leastLoadedNode。

第二阶段：JOIN_GROUP

发起 `JoinGroupRequest` 请求。如果配置了多种策略，那么其中就会包含多个 protocol_name protocol_metadata。

```Golang
protocol_name 对应 ProtocolName() 方法
protocol_metadata 对应 UserData() 方法
```

**注意**，如果是原有的消费者重新加入消费组，那么在真正发送 oinGroupRequest 请求之前还要执行一些准备工作：

- 如果费端参数 开启自动提交位移功能，那么在请求加入消费组之前需要向 GPC 提交消费位移。**这个过程是阻塞执行的，要么成功提交消费位移，要么超时。**
- 如果消费者添加了自定义的再均衡监听器，那么此时会调用 AssignGroups() 方法在重新加入消费组之前实施自定义的规则逻辑，比如清除一些状态，或者提交消费位移等。
- 因为是重新加入消费组，之前与 GPC 节点之间的心跳检测也就不需要了，所以在成功地重新加入消费组之前需要禁止心跳检测的运作。

#### 随意的消费者leader选举

情况1: 没有leader

选第一个加入消费者组的消费者

情况2: 有leader，但退出了

消费者信息以HashMap形式存储，key是消费者的member_id，value是消费者的相关元数据。leaderId表示leader消费者的member_id，它的取值为HashMap中的第一个kv对的key。这么来看和随机无异。

综上很随意...

#### 投票得来的分区分配策略

每个消费者都可以设置自己的分区分配策略，对消费组而言需要从各个消费者呈报上来的各个分配策略中选举一个彼此都"信服"的策略来进行整体上的分区分配。

这个分区分配的选举并非由 leader 消费者决定，而是根据消费组内的各个消费者投票来决定的。**注意，这里不需要 GPC 再和各个消费者发生通信，而是根据各个消费者呈报的分配策略来实施。**

最终选举的分配策略基本上可以看作被各个消费者支持的最多的策略，具体的选举过程如下：
- 收集各个消费者支持的所有分配策略，组成候选集 candidates
- 每个消费者从候选集 candidates 找出第 个自身支持的策略，为这个策略投上一票。
- 计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。

尤其注意：
- 如果有消费者并不支持选出的分配策略，会报出异常。
- "消费者所支持的分配策略"是 `partition.assignment.strategy` 参数配置的策略。也就是说：假设如果这个参数值只配置了RangeAssignor，那么这个消费者客户端只支持 RangeAssignor 分配策略，而不是消费者客户端代码中实现的3种分配策略及可能的自定义分配策略。

#### 两个图概述一 二阶段

![](/images/mq/kafka/kafka24.png)

![](/images/mq/kafka/kafka25.png)

第三阶段：SYNC_GROUP

leader 费者根据在第二阶段中选举的分区分配策略来实施具体的分区分配，在此之后需要将分配的方案同步给其他各个消费者。

这个同步的过程，是通过 GPC 来负责转发同步分配方案的。

![](/images/mq/kafka/kafka26.png)

第四阶段：HEARTBEAT

进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。

在正式消费之前，消费者还需要确定拉取消息的起始位置。

假设之前已经将最后的消费位移提交到了 GPC 并且 GPC 将其保存到了 Kafka内部的 __consumer_offsets 主题中，此时消费者可以通过 OffsetFetchRequest 请求获取上次提交的 offset 并从此处继续消费。